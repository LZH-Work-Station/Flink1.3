# 状态一致性

[TOC]



## 1. 什么是状态一致性

状态一致性就是保证数据，被处理一次，而且不会被重复处理

## 2. 状态一致性的分类

### (1). 最多一次

丢数据也不怕，级别最低

### (2). 至少一次

数据有可能被重复处理，但是数据不会丢。幂等性的数据只需要满足至少一次就可以保证数据的准确性。例如：UV

#### 幂等性

当相同的数据重复到达不会影响最后的结果就具有幂等性。例如UV，用来计算当天访问的次数（相同用户不会重复计算）。

### (3). 精确一次

数据恰好被处理一次

## 3. 为什么checkpoint无法保证精确一次

因为如果数据被消费了，但是检查点没有及时的保存，就会出现重复消费的情况

## 4. 怎么保证精确一次

- 开启checkpoint

- source端不用做任何处理，因为只要我们保证数据处理的过程中重复数据不会被重复sink就可以了

- sink端要采用下面操作来保证数据不会重复写入外部系统

  - 幂等写入（同一个操作可以重复执行多次，但是只有一次导致结果更改）

    例如写入redis，我们采用key value的形式写入，那么重复的数据不会影响结果，只会更新结果

    问题：**会出现数据的回退，例如数据是10，15，20，重复消费后，数值可能又变回10，15，20然后回退到15，20**。虽然最后的结果不变，但是会产生数据的延迟

  - 事务写入（需要外部系统提供事务性保证）

    构建的事务对应着checkpoint，等到checkpoint真正完成的时候，才把所有对应的结果写入到sink系统中。

### Flink和Kafka连接时的精确一致性保证

- 开启检查点
- 在FlinkKafkaConsumer中将当前读取的偏移量保存为算子状态（API已帮我们完成）

- 当第一条数据到来时，或者收到检查点的分界线时，Sink 任务都会启动一个事务。 
- 接下来接收到的所有数据，都通过这个事务写入外部系统；这时由于事务没有提交，所以数据尽管写入了外部系统，但是不可用，是“预提交”的状态。 
- 当 Sink 任务收到 JobManager 发来检查点完成的通知时，正式提交事务，写入的结果就 真正可用了。

实践：

（1）必须启用检查点

（2）在 FlinkKafkaProducer 的构造函数中传入参数 Semantic.EXACTLY_ONCE 

（3）配置 Kafka 读取数据的消费者的隔离级别 这里所说的 Kafka，是写入的外部系统。预提交阶段数据已经写入，只是被标记为“未提 交”（uncommitted），而 Kafka 中默认的隔离级别 isolation.level 是 read_uncommitted，也就是 可以读取未提交的数据。这样一来，外部应用就可以直接消费未提交的数据，对于事务性的保 证就失效了。所以应该将隔离级别配置 为 read_committed，表示消费者遇到未提交的消息时，会停止从分区中消费数据，直到消 息被标记为已提交才会再次恢复消费。当然，这样做的话，外部应用消费数据就会有显著的延 迟

​			

### 使用唯一id来保证数据只消费一次

对数据产生唯一id，然后每次sink的时候看看外部系统里面有没有这个id，如果有就代表已经消费过了，就不再重复消费